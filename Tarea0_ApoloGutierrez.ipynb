{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"20%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "<h1 align='center'> INF-393 Máquinas de Aprendizaje II-2019 </h1>\n",
    "\n",
    "<H3 align='center'> Tarea 0 - Introducción a Máquinas de Aprendizaje </H3>\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "\n",
    "**Temas**  \n",
    "* Introducción a librerías comunes de *Machine Learning*:\n",
    "    * Pandas\n",
    "    * Numpy\n",
    "    * Sklearn\n",
    "    * Matplotlib\n",
    "    * Otro..\n",
    "* Implementación de Perceptrón y variantes.\n",
    "* Implementación de método aprendizaje online (Gradiente descendente).\n",
    " \n",
    "\n",
    "** Formalidades **  \n",
    "* Equipos de trabajo de: 2 personas\n",
    "* Se debe preparar un (breve) Jupyter/IPython notebook que explique la actividad realizada y las conclusiones del trabajo\n",
    "* Fecha de entrega: 6 de Septiembre.\n",
    "* Formato de entrega: envı́o de link Github al correo electrónico del ayudante (*<francisco.mena.13@sansano.usm.cl>*) , incluyendo al profesor en copia (*<jnancu@inf.utfsm.cl>*). Por favor especificar el siguiente asunto: [Tarea0-INF393-II-2019]\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "\n",
    "La tarea se divide en secciones:\n",
    "\n",
    "[1.](#primero) Perceptrón a mano\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn as skl\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"primero\"></a>\n",
    "## 1. Perceptrón a mano\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1838/1*n6sJ4yZQzwKL9wnF5wnVNg.png\" width=\"40%\"  />\n",
    "\n",
    "En esta sección se le pedirá que implemente el algoritmo online del *perceptrón* [[2]](#refs) para aprender una función de separación lineal en un problema de clasificación binaria (0 o 1) a través de la función de *treshold*. Un algoritmo online, como el caso del *perceptrón*, aprende de una instancia de dato a la vez $(x^{(i)},y^{(i)})$, dentro de un conjunto de datos $\\{(x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}), \\ldots, (x^{(N)},y^{(N)})  \\}$, donde la predicción de la clase para cada instancia es través de la función de *treshold*:\n",
    "\n",
    "$$\n",
    "\\hat{y_i} = f(x^{(i)};w,b) = \\left\\{ \\begin{array}{lc}\n",
    "       1 &  si \\ \\sum_j w_j \\cdot x^{(i)}_j +b \\geq \\theta \\\\\n",
    "       0 &  si \\ \\sum_j w_j \\cdot x^{(i)}_j +b < \\theta\n",
    "     \\end{array} \\right.\n",
    "$$\n",
    "\n",
    "\n",
    "Donde $\\theta = 0$. Recordar que el *bias* $b$ se puede incluir dentro de los pesos/parámetros $w$ si se agrega una columna extra de 1's a los datos de entrada $x$ (*como se ve en la imagen anterior*). \n",
    "\n",
    "Para lo que sigue de la actividad sólo podrá utilizar *numpy* (para operaciones de algebra lineal).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> a) Escriba una función que calcule el valor de salida (*output*) del modelo $f(x)$ para un patrón de entrada $x$ a través de los pesos $w$ del modelo. *Decida si incluir los bias dentro de $w$ o manejarlos de manera separada*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(x, W, bias):\n",
    "    \n",
    "    suma_producto = np.dot(x, W) \n",
    "    if suma_producto >= -(bias):          \n",
    "        Y_predict = 1\n",
    "    else:          \n",
    "        Y_predict = 0\n",
    "    return Y_predict\n",
    "\n",
    "def prediction_X(X,W,bias):\n",
    "    \n",
    "    Y = []\n",
    "    for x in X:\n",
    "        result = prediction(x,W,bias)\n",
    "        Y.append(result)\n",
    "    return np.array(Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> b) Escriba una función que implemente el clásico algoritmo del **Perceptrón** para un problema binario que permita entrenarlo en un conjunto de datos de tamaño $N$, leídos de manera *online* (uno a uno). *Recordar la decisión anterior sobre los bias*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se aplica el algoritmo del perceptrón para ajustar los valores de W\n",
    "def fit_w(X, Y, lr, loops):\n",
    "    W = np.zeros(X.shape[1])\n",
    "    b = 0\n",
    "    W_return = []\n",
    "    acc = {}\n",
    "    max_acc = 0\n",
    "    \n",
    "    for i in range(loops):\n",
    "        for x, y in zip(X, Y):\n",
    "            y_pred = prediction(x,W,b)\n",
    "            if y == 1 and y_pred == 0:\n",
    "                W = W + lr * x\n",
    "                b = b - lr * 1\n",
    "            elif y == 0 and y_pred == 1:\n",
    "                W = W - lr * x\n",
    "                b = b + lr * 1\n",
    "\n",
    "        W_return.append(W)    \n",
    "    \n",
    "        acc[i] = skl.metrics.accuracy_score(prediction_X(X,W,b), Y)\n",
    "        if (acc[i] > max_acc):\n",
    "            max_acc = acc[i]\n",
    "            j = i\n",
    "            aux_w = W\n",
    "            aux_b = b\n",
    "\n",
    "    W = aux_w\n",
    "    b = aux_b\n",
    "\n",
    "    print(max_acc,j)\n",
    "\n",
    "    plt.plot(acc.values())\n",
    "    plt.xlabel(\"Número de iteraciones\")\n",
    "    plt.ylabel(\"Precisión\")\n",
    "    plt.ylim([0, 1])\n",
    "    plt.show()\n",
    "\n",
    "    return np.array(W_return)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> c) Demuestre que lo implementado funciona en un problema real de clasificación. Para esto utilice el dataset **Breast cancer wisconsin**, disponible a través de la librería __[*sklearn*](http://scikit-learn.org)__, el cual corresponde a la detección de cancer mamario a través de características relevantes (numéricas continuas) de un examen realizado, como por ejemplo la textura, simetría y tamaño de una masa mamaria. Estas características deben combinarse linealmente para la detección del cancer.\n",
    "> <div class=\"alert alert-block alert-info\">Es una buena práctica el normalizar los datos antes de trabajar con el modelo</div>\n",
    "```python\n",
    "\n",
    "X_train,y_train = load_breast_cancer(return_X_y=True)\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_train = np.c_[X_train, np.ones(N) ] #add columns of 1's if you want\n",
    "```\n",
    "Para evaluar los resultados mida la exactitud (*accuracy*) de la clasificación durante el entrenamiento (por cada iteración/instancia/dato) y grafique, utilice el conjunto de entrenamiento realizando una sola pasada (el objetivo de esta sección es familiarizarse con el algoritmo). Además reporte el tiempo de entrenamiento mediante el algoritmo implementado.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8060734da3b4bc9a17dbb5f3c6f7c68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>interactive</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='lr', max=1.0), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.fit_w>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train,y_train = load_breast_cancer(return_X_y=True)\n",
    "scaler = skl.preprocessing.StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "interact(fit_w, X = fixed(X_train), Y = fixed(y_train), lr=(0,1,0.1), loops=fixed(10000))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> d) Escriba una función que implemente el **Forgetrón** [[3]](#refs) con una memoria de tamaño $K$ y la función de kernel como el producto interno (*inner-product*), esto es $<a,b> = \\sum_i a_i \\cdot b_i$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> e) Vuelva a realizar el item c) para el **Forgetrón** con un $K=10$ y compare los resultados.\n",
    "\n",
    "\n",
    "### ¿Qué sucede al variar la función objetivo del problema? \n",
    "Si utilizáramos la función de pérdida *binary cross entropy*, que castiga de manera suave los valores en que se equivoca el modelo a través de que el valor de salida sea una confiabilidad $g(x; w,b) \\in [0,1]$.\n",
    "$$\n",
    "\\ell (y, \\ g(x;w,b)) = - y \\cdot \\log{(g(x;w,b))} - (1-y) \\cdot \\log{(1-g(x;w,b))}\n",
    "$$\n",
    "\n",
    "Realice una modificación al perceptrón para que entregue como salida una confiabilidad continua entre 0 y 1. Una buena aproximación de la función *treshold* (con $\\theta=0$) del perceptrón es la función sigmoidal.\n",
    "\n",
    "<img src=\"https://i.imgur.com/lr6F3Ur.png\" width=\"60%\"  />\n",
    "\n",
    "Ésto sería modelar el perceptrón como:\n",
    "$$\n",
    "g(x^{(i)};w,b) = p(y=1|x^{(i)}) = \\sigma \\left( \\sum_j w_j \\cdot x^{(i)}_j +b \\right)\n",
    "$$\n",
    "\n",
    "Con $\\sigma$ la función sigmoidal de la forma $\\sigma(\\xi) = 1/(1+e^{-\\xi}) $, la cual tiene una derivada cíclica que hace más fácil el cálculo: $\\sigma'(\\xi) = \\sigma(\\xi) (1-\\sigma(\\xi))$\n",
    "\n",
    "> f) Escriba una función que compute la función sigmoidal para una entrada $\\xi$ cualquiera. *Tenga cuidado con los límites de números que puede trabajar python (por ejemplo $\\exp{800}\\rightarrow +\\infty$)*. *Se aconseja acotar/truncar los valores que entran a la función para que la operación se pueda realizar*. Además escriba una función que calcule la salida del nuevo modelo $g(x; w,b)$ con esta función sigmoidal.\n",
    "\n",
    "> g) Escriba una función que calcule la función de pérdida descrita anteriormente para un dato $x^{(i)}$, utilizando $g(x^{(i)};w,b)$. *Tenga cuidado con los límites del logaritmo (recordad que $\\log{0}\\rightarrow +\\infty$)*.\n",
    "\n",
    "> h) Escriba una función que calcule el gradiente (derivada) de la función de pérdida anterior con respecto a los pesos del modelo $w$. *Se recomienda derivarla analíticamente y luego escribirla*. *Recuerde el uso de la regla de la cadena*.\n",
    "\n",
    "> i) Realice una modificación al algoritmo implementado en b) (**Perceptrón**) para que se adapte a la función objetivo *binary cross entropy* implementada, para ésto haga uso del algoritmo de optimización SGD [[4]](#refs) (*Stochastic Gradient Descend*) con tasa de aprendizaje $\\eta \\in [0,1]$.\n",
    "\n",
    "$$ \\vec{w}^{(t+1)} \\leftarrow \\vec{w}^{(t)} - \\eta \\cdot \\nabla_{\\vec{w}^{(t)}} \\ell $$\n",
    "\n",
    "> j) Vuelva a realizar el item c) con esta modificación, además grafique la función de pérdida en el transcurso del entrenamiento. Compare los resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"refs\"></a>\n",
    "### Referencias\n",
    "[1] Hastie, T.; Tibshirani, R., Friedman, J. (2009), *The Elements of Statistical Learning*, Second Edition.\n",
    "Springer New York Inc.  \n",
    "[2] STEPHEN, I. (1990). *Perceptron-based learning algorithms*. IEEE Transactions on neural networks, 50(2), 179.  \n",
    "[3] Dekel, O., Shalev-Shwartz, S., & Singer, Y. (2006). *The Forgetron: A kernel-based perceptron on a fixed budget*. In Advances in neural information processing systems (pp. 259-266).  \n",
    "[4] Ruder, S. (2016). *An overview of gradient descent optimization algorithms*. arXiv preprint arXiv:1609.04747.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
